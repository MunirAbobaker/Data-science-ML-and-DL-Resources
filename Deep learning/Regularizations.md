## [An Overview of Regularization Techniques in Deep Learning](https://www.analyticsvidhya.com/blog/2018/04/fundamentals-deep-learning-regularization-techniques/)

**in machine learning, you will have a fair idea that regularization penalizes the coefficients. In deep learning, it actually penalizes the weight matrices of the nodes.**

Regularization in deep learning is a technique used to prevent overfitting and improve the generalization of neural networks. It involves adding a regularization term to the loss function, which penalizes large weights or complex model architectures. Regularization methods such as L1 and L2 regularization, dropout, and batch normalization help control model complexity and improve its ability to generalize to unseen data.

- [Keras Regularizers](https://keras.io/api/layers/regularizers/)
- [Tensorflow regularizers](https://www.tensorflow.org/api_docs/python/tf/keras/regularizers/Regularizer)
- [Pytoch Regularizers](https://stackoverflow.com/questions/42704283/l1-l2-regularization-in-pytorch)

We have several regularization tools at our end, some of them are 
- early stopping
- dropout
- weight initialization techniques
- batch normalization.
- L2 & L1 regularization
- Data Augmentation

 
