# Table of Contents
1. [NLP](#nlp)
   - [UMass CS685: Advanced Natural Language Processing (Spring)](#umass-cs685-advanced-natural-language-processing-spring)
   - [NLP Course Notes and Videos](#nlp-course-notes-and-videos)
   - [Resources for CNN, ML, NLP](#resources-for-cnn-ml-nlp)
   - [Curated List of Papers on NLP](#curated-list-of-papers-on-nlp)
   - [Notebooks](#notebooks)
2. [Transformers](#transformers)
   - [Resources for Transformers](#resources-for-transformers)
   - [Notebooks](#notebooks-1)
3. [Portfolio](#portfolio)
   - [A curated list of awesome GitHub Profile READMEs](#a-curated-list-of-awesome-github-profile-readmes)
   - [How to make an impressive Data Science Portfolio?](#how-to-make-an-impressive-data-science-portfolio)


# NLP

![pnlp_0101](https://user-images.githubusercontent.com/110838853/226782983-768b069e-52ed-4442-8909-d9553ab8b61a.png)

NLP stands for natural language processing, which is a branch of artificial intelligence that focuses on the interaction between computers and human language. It involves the development of algorithms and models that enable computers to understand, interpret, and generate human language.

NLP has many applications, including:

- Text classification: The process of categorizing text into one or more predefined categories, such as sentiment analysis (determining whether a piece of text expresses a positive or negative sentiment) or topic classification (categorizing text into topics such as politics, sports, or entertainment).
- Machine translation: The process of automatically translating text from one language to another.
- Named entity recognition: The process of identifying and extracting entities such as people, places, and organizations from text.
- Sentiment analysis: The process of determining the sentiment expressed in a piece of text, whether it is positive, negative, or neutral.
- Question answering: The process of answering natural language questions posed by humans.

NLP involves a range of techniques, including statistical models, rule-based models, and deep learning models such as recurrent neural networks (RNNs) and transformers. These models are trained on large datasets of annotated text to learn the patterns and structures of language, and are used to perform a wide range of tasks in natural language processing.

### UMass CS685: Advanced Natural Language Processing (Spring)
- [YouTube Playlist](https://www.youtube.com/playlist?list=PLWnsVgP6CzadI4-FT2Po4wsEK7MHCIQ-d)

### NLP Course Notes and Videos
- [GitHub Repository](https://github.com/ashishpatel26/ML-Course-Notes)

### Resources for CNN, ML, NLP
- [GitHub Repository](https://github.com/ashishpatel26/ResourceBank_CV_NLP_MLOPS_2022) - This repository offers a goldmine of materials for computer vision, natural language processing, and machine learning operations.
- [To Know all about Transformers](https://github.com/dair-ai/Transformers-Recipe)
### Curated List of Papers on NLP
- [GitHub Repository](https://github.com/dair-ai/nlp_paper_summaries)

### Notebooks 
- [GitHub Repository](https://github.com/dair-ai/Mathematics-for-ML)
- [Ml and NLP ,LLm,CV Notebooks](https://github.com/dair-ai/ML-Notebooks)

# Transformers

Transformers are a type of machine learning architecture used primarily for natural language processing tasks such as language translation, sentiment analysis, and text classification. The Transformer architecture was first introduced in a 2017 paper called "Attention is All You Need" by Vaswani et al.

Traditionally, recurrent neural networks (RNNs) have been used for sequence-to-sequence tasks, but Transformers have gained popularity due to their ability to handle long-range dependencies in sequences more efficiently.

Transformers rely on self-attention mechanisms to determine which parts of a sequence are most relevant to each other. Self-attention allows the model to weigh the importance of different words or tokens in a sequence when generating an output.

Transformers have been used with great success in various natural language processing tasks and have even been applied to other domains such as image and audio processing. The pre-trained transformer models such as BERT, GPT-3, and T5 have achieved state-of-the-art results on a wide range of natural language processing benchmarks.


### Resources for Transformers
- [Study Guide to learn Transformers](https://github.com/dair-ai/Transformers-Recipe)
- [GitHub Repository](https://github.com/ashishpatel26/Treasure-of-Transformers) - Awesome Treasure of Transformers Models for Natural Language processing contains papers, videos, blogs, official repo along with colab Notebooks.
- [Transformers Course Notes and Videos](https://github.com/ashishpatel26/ML-Course-Notes)
- [Explanation of Transformers with Code by  MadewithML](https://madewithml.com/courses/foundations/transformers/)
- [GitHub Repository for Transformers papers](https://github.com/dair-ai/ML-Papers-Explained)
- [Medium Article](https://medium.com/nlplanet/two-minutes-nlp-20-learning-resources-for-transformers-1bbff88b7524) - 20 Learning Resources for Transformers

### Notebooks 
- [GitHub Repository](https://github.com/dair-ai/Mathematics-for-ML)

# Portfolio

### A curated list of awesome GitHub Profile READMEs
- [GitHub Repository](https://github.com/abhisheknaiidu/awesome-github-profile-readme)

### How to make an impressive Data Science Portfolio?
- [Analytics Vidhya](https://www.analyticsvidhya.com/blog/2021/04/how-to-make-an-impressive-data-science-portfolio/)
