# Deep Learning Model Architecture Variables([Intiation of Neural netoworks](https://www.deeplearning.ai/ai-notes/initialization/index.html))

This repository provides an overview of the different variables that can affect deep learning model architecture, along with their subtypes.

## Model Architecture Variables

1. **Neural Network Layers**
   - Convolutional Layers
     - 1D Convolutional Layer
     - 2D Convolutional Layer
     - 3D Convolutional Layer

   - Recurrent Layers
     - Simple Recurrent Layer
     - Gated Recurrent Unit (GRU) Layer
     - Long Short-Term Memory (LSTM) Layer

   - Dense (Fully Connected) Layers

3. Layer Architecture:
   - Number of Layers
   - Layer Connectivity
   - Layer Type

4. Neuron Configuration:
   - Number of Neurons
   - Activation Functions
   
5. **Activation Functions**
   - Sigmoid Activation
   - Tanh Activation
   - Rectified Linear Unit (ReLU) Activation
   - Leaky ReLU Activation
   - Parametric ReLU Activation
   - Exponential Linear Unit (ELU) Activation

6. **Regularization Techniques**
   - L1 Regularization (Lasso)
   - L2 Regularization (Ridge)
   - Dropout
   - Batch Normalization

7. **Optimizers**
   - Stochastic Gradient Descent (SGD)
   - Adam Optimizer
   - Adagrad Optimizer
   - RMSprop Optimizer

8. **Learning Rate**
   - Constant Learning Rate
   - Learning Rate Decay
   - Adaptive Learning Rate

9. [**Loss Functions**](https://machinelearningmastery.com/how-to-choose-loss-functions-when-training-deep-learning-neural-networks/),[Article](https://www.analyticsvidhya.com/blog/2022/06/understanding-loss-function-in-deep-learning/)
   - Mean Squared Error (MSE)
   - Binary Cross-Entropy
   - Categorical Cross-Entropy
   - Kullback-Leibler Divergence

10. [**Model Initialization**](https://datascience.stackexchange.com/questions/37378/what-are-kernel-initializers-and-what-is-their-significance) [Article](https://www.deeplearning.ai/ai-notes/initialization/index.html)
   - Random Initialization
   - Xavier/Glorot Initialization
   - He Initialization

11. **Batch Size**
   - Fixed Batch Size
   - Mini-Batch Size
   - Batch Size Scaling

12. **Network Depth**
   - Shallow Networks
   - Deep Networks
   - Very Deep Networks

13. **Network Width**
    - Narrow Networks
    - Wide Networks

14. **Skip Connections**
    - Residual Connections
    - Highway Connections

15. **Pooling Layers**
    - Max Pooling
    - Average Pooling

16. **Input Preprocessing**
    - Normalization
    - Data Augmentation

17. **Model Ensemble**
    - Voting Ensemble
    - Bagging Ensemble
    - Boosting Ensemble

18. Hyperparameters:
   - Learning Rate
   - Batch Size
   - Epochs
   - Dropout Rate
   - Filter Sizes
  
19. **Input Preprocessing**
    - Normalization
    - Data Augmentation

20. **Model Ensemble**
    - Voting Ensemble
    - Bagging Ensemble
    - Boosting Ensemble

21. **Neural Network Layers**
   - Graph Convolutional Layers
   - Transformer Layers
   - Capsule Layers

22. **Normalization Techniques**
   - Layer Normalization
   - Instance Normalization
   - Group Normalization

23. **Attention Mechanisms**
   - Spatial Attention
   - Channel Attention
   - Multi-Head Attention

24. **Gradient Optimization**
   - Gradient Clipping
   - Weight Decay
   - Learning Rate Scheduling

25. **Memory Management**
   - Memory-Augmented Networks
   - Differentiable Neural Computers (DNC)

26. **Loss Functions**
   - Mean Absolute Error (MAE)
   - Huber Loss
   - Contrastive Loss

27. **Data Augmentation Techniques**
   - Random Rotation
   - Random Translation
   - Random Scaling

28. **Model Interpretability**
   - Feature Importance
   - Grad-CAM

29. **Recurrent Architectures**
   - Bidirectional RNNs
   - Stacked RNNs
   - Attention-Based RNNs

30. **Advanced Pooling Techniques**
    - Global Average Pooling
    - Adaptive Pooling

31. **Model Compression**
    - Pruning
    - Quantization
    - Knowledge Distillation

32. **Hyperparameter Optimization**
    - Grid Search
    - Random Search
    - Bayesian Optimization

33. **Transfer Learning**
    - Feature Extraction
    - Fine-Tuning

34. **Attention Mechanisms**
    - Self-Attention
    - Transformer Attention

35. **Reinforcement Learning**
    - Policy Gradient Methods
    - Q-Learning

36. **Memory Networks**
    - Differentiable Neural Computers (DNC)
    - Neural Turing Machines (NTM)

37. **Initialization Methods**
    - Random Initialization
    - Xavier/Glorot Initialization
    - He Initialization

38. **Learning Rate Scheduling**
    - Step Decay
    - Exponential Decay
    - Cyclical Learning Rates

39. **Model Regularization Techniques**
    - Dropout
    - L1 Regularization (Lasso)
    - L2 Regularization (Ridge)

40. **Network Architecture Modifications**
    - Skip Connections (Residual, Highway)
    - DenseNet
    - Inception Modules

41. **Model Compression Techniques**
    - Pruning
    - Quantization
    - Knowledge Distillation

42. **Memory and Computational Efficiency**
    - Low-Rank Approximation
    - Tensor Decomposition
    - Parameter Sharing

43. **Transfer Learning**
    - Feature Extraction
    - Fine-Tuning

44. **Data Preprocessing**
    - Normalization
    - One-Hot Encoding
    - Tokenization

45. **Handling Imbalanced Data**
    - Oversampling
    - Undersampling
    - SMOTE (Synthetic Minority Over-sampling Technique)

46. **Handling Sequential Data**
    - Recurrent Neural Networks (RNN)
    - Long Short-Term Memory (LSTM)
    - Gated Recurrent Unit (GRU)

47. **Interpretability and Explainability**
    - Feature Importance (SHAP, LIME)
    - Attention Mechanisms
    - Model Visualization

48. **Hardware Considerations**
    - GPU Acceleration
    - Distributed Training
    - Model Parallelism

49. **Task-specific Considerations**
    - Object Detection
    - Natural Language Processing (NLP)
    - Time Series Analysis
