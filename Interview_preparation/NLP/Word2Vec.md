
### [for More Info watch](https://www.youtube.com/watch?v=Q95SIG4g7SA&list=PLZoTAELRMXVMdJ5sqbCK2LiM0HhQVWNzm&index=31) or Go throgh this [blog](https://jalammar.github.io/illustrated-word2vec/),[Analytics_Vidya Blog](https://www.analyticsvidhya.com/blog/2021/07/word2vec-for-word-embeddings-a-beginners-guide/)

## Word2Vec (CBOW)
- Each word in a given vocabulary is represented as a vector in a high-dimensional space. The dimensionality of the vectors is typically chosen based on the desired window size.

- The model consists of an input layer, a hidden layer, and an output layer. The input layer represents the context words (words surrounding the target word), and the output layer represents the target word.

- Each word in the context is converted into a vector representation using the word embeddings.

- The vectors of the context words are fed into the model as input. The model aims to predict the target word based on the context words.

- The hidden layer receives the input vectors and performs computations using weights associated with each connection between nodes. Each node in the hidden layer is connected to every node in the input layer.

- The output layer produces a vector representation of the target word using weights connecting the hidden layer nodes to the output layer nodes.

- The model is trained by adjusting the weights to minimize the difference between the predicted output and the actual target word. This is done by calculating a loss function, which measures the dissimilarity between the predicted output and the true output.

- Forward propagation refers to the process of passing the input vectors through the model and obtaining the predicted output.

- Backward propagation involves adjusting the weights in the model based on the calculated loss. The weights are updated in a way that reduces the loss and improves the model's performance.

- The training process continues iteratively, adjusting the weights and minimizing the loss until the model reaches a state where the predicted output is close to the true output.

- Once the model is trained, the vector representations of the words in the vocabulary (including the target word) can be extracted from the hidden layer or the output layer. These vector representations capture semantic and syntactic similarities between words.

- The resulting vectors can be used for various natural language processing tasks, such as word similarity calculations, text classification, or even as input to other machine learning models.


![Screenshot from 2023-07-07 16-13-49](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/d74790cc-f7ed-44d0-820d-0965e6541186)

![Screenshot from 2023-07-07 17-39-54](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/c07c7e4a-7316-400e-8c0b-a71e477b30b9)

![Screenshot from 2023-07-07 17-43-43](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/b98120b0-a099-4394-a325-ca218137cef7)
![Screenshot from 2023-07-07 17-48-56](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/585e80ac-3262-414d-9ce2-e3453a68d93a)


![Screenshot from 2023-07-07 17-51-37](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/502df1bf-b4ea-435c-bb5b-98dd74de870f)

## Summary of CBOW Word2Vec
To summarize the process from neural network inputs to outputs and how words are converted into vectors in the context of word2vec and continuous bag-of-words (CBOW) approach, here are the key points:

Neural Network Structure: The neural network used in word2vec typically consists of an input layer, one or more hidden layers, and an output layer.

Connection between Nodes: Each node in the input layer is connected to every node in the hidden layers. Similarly, each node in the hidden layers is connected to every node in the output layer. These connections represent the initialized weights in the network.

Word-to-Vector Conversion: Words are converted into vectors by utilizing a sliding window approach. The window size determines the number of words considered in the context of the target word. For example, if the window size is 5, each word will be represented by a vector of length 5.

Forward Propagation: During forward propagation, a target word and its context words (within the defined window) are passed as input to the neural network. The network processes the input through the connections and calculates an output.

Output Representation: The output layer of the neural network produces a vector representation of the target word. The size of this vector is determined by the window size chosen. Each element of the vector represents a specific feature or dimension of the word.

Loss Calculation: The true output vector for the target word is compared to the predicted output vector generated by the neural network. A loss function (e.g., cross-entropy loss) is used to measure the difference between the predicted and true outputs.

Backward Propagation: Backward propagation involves updating the weights in the neural network based on the calculated loss. The goal is to minimize the loss by adjusting the weights through optimization techniques such as gradient descent.

Iterative Training: The training process involves repeatedly feeding different target words and their context words into the neural network, calculating the loss, and updating the weights. This iteration continues until the model achieves a satisfactory level of performance, where the predicted outputs (word vectors) are close to the true outputs.

Vector Representation: After training, the vectors obtained for each word in the vocabulary serve as feature representations. Each word is represented by a vector of fixed dimensions (determined by the window size) that captures semantic relationships and contextual information.

By training the word2vec model on a large corpus, the word vectors can capture useful semantic and syntactic information, enabling various natural language processing tasks such as word similarity, word analogy, and even powering downstream applications like document classification or machine translation.





Regenerate response




## Genism

## Word2vec

Word2vec is a technique for natural language processing (NLP) published in 2013. The word2vec algorithm uses a neural network model to learn word associations from a large corpus of text. Once trained, such a model can detect synonymous words or suggest additional words for a partial sentence.

  - Keep the Semantic meaning of DIfferent Words by representing each words as a vector of  32 or more dimensional instead of single  value number (BOW and TF-IDF Model)
  - ![Screenshot from 2023-06-14 14-51-26](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/0d55eff4-b150-4c9d-ae47-d2bb43c790dd)

  - ![Screenshot from 2023-06-14 14-48-49](https://github.com/chethanhn29/Personal-Collection-of-Resources-to-learn/assets/110838853/123b05cb-48fc-47ed-a822-f48723ef2e7c)

![](https://miro.medium.com/v2/resize:fit:678/1*5F4TXdFYwqi-BWTToQPIfg.jpeg)
![](https://www.samyzaf.com/ML/nlp/word2vec2.png)

## Avg Wordvec



