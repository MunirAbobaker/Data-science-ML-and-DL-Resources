
# Resources for Learning Generative AI

## Online Courses
- [Introduction to LangChain on Udemy](https://www.udemy.com/course/introduction-to-langchain/learn/lecture/39848376?start=0#overview)
- [LLM Bootcamp Spring 2023: Prompt Engineering](https://fullstackdeeplearning.com/llm-bootcamp/spring-2023/prompt-engineering/)
- [Deeplearning.ai short courses on LLM](https://www.deeplearning.ai/short-courses/langchain-for-llm-application-development/)
-  [Coursera Courses]
- [Awesome-LLM Github Repo](https://github.com/Hannibal046/Awesome-LLM)
- [Hands on LLM Github Repo](https://github.com/iusztinpaul/hands-on-llms)
- [LLM Course mlabonne Github](https://github.com/mlabonne/llm-course)
- [Nvidia LLM Course](https://www.nvidia.com/en-in/training/)
- [Weights and Bias LLM Courses](https://www.wandb.courses/pages/w-b-courses),[2](https://www.wandb.courses/courses/building-llm-powered-apps)
- [Weights and Biases LLM Finetuning](https://www.wandb.courses/courses/training-fine-tuning-LLMs)
- [Krish Naik]
- [Nicholas Renotte]
- [Data Bricks](
- [Free Generative AI & Large Language Models Courses by aciveloop](https://learn.activeloop.ai/)
- [COhere Course](https://docs.cohere.com/docs/the-cohere-platform)
- Hugging Face NLP Course
- [ LLM Notebooks ](https://mlabonne.github.io/blog/posts/2023-06-07-Decoding_strategies.html)
- [RAG From Scracth by Langchain](https://www.youtube.com/watch?v=wd7TZ4w1mSw&list=PLfaIDFEXuae2LXbO1_PKyVJiQ23ZztA0x)
- 1️⃣ Building LLM Applications for Production
It’s easy to build LLMs, but very hard to make something production-ready with them. This article by Chip Huyen covers how to put LLMs into production. 
➡ Link: https://lnkd.in/g8wt5CeG

2️⃣ Awesome LLMOps GitHub Repo
This GitHub repo contains a curated list of the best LLMOps resources and tools for developers. 
➡ Link: https://lnkd.in/dpbzGWFv
 
3️⃣ LLMOps Course 
In this course, you’ll go through the LLMOps pipeline of pre-processing training data for supervised instruction tuning, and adapt a supervised tuning pipeline to train and deploy a custom LLM. 
➡ Link: https://lnkd.in/dJhagzi4

4️⃣ Automated Testing for LLMOps Course 
In this course, you will learn how to create a continuous integration (CI) workflow to evaluate your LLM applications at every change for faster, safer, and more efficient application development.

## 
➡ Link: https://lnkd.in/dqzpAbQn
𝐋𝐋𝐌𝐬 𝐑𝐨𝐚𝐝𝐦𝐚𝐩: A curated list of contents from #nlp fundamentals to understanding #llms .

📌 𝐍𝐋𝐏
📚 𝐂𝐨𝐮𝐫𝐬𝐞: NLP specialization https://lnkd.in/eZQc9hTu
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: NLP word representations basics: https://lnkd.in/eFqSJYxa
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: Attention is all you need: https://lnkd.in/erP6xex8
📹 𝐕𝐢𝐝𝐞𝐨: Word embedding, Word2Vec: https://lnkd.in/eweB-sjU
📹 𝐕𝐢𝐝𝐞𝐨: Decoder only Transformers: https://lnkd.in/e-ewtNXJ

📌 𝐋𝐚𝐫𝐠𝐞 𝐋𝐚𝐧𝐠𝐮𝐚𝐠𝐞 𝐌𝐨𝐝𝐞𝐥𝐬
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Understanding Large Language Models https://lnkd.in/ehjGtA8U
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Hands on LLM course by Maxime Labonne https://lnkd.in/e_xNSZyY
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Hans on LLM by Paul Iusztin and Pau Labarta Bajo https://lnkd.in/eevAYGky
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: LLM Patterns: https://lnkd.in/eJT3m9Ck

📌 𝐅𝐨𝐮𝐧𝐝𝐚𝐭𝐢𝐨𝐧 & 𝐏𝐫𝐞-𝐭𝐫𝐚𝐢𝐧𝐞𝐝 𝐌𝐨𝐝𝐞𝐥𝐬
📚 𝐂𝐨𝐮𝐫𝐬𝐞: NLP course Hugging Face https://lnkd.in/e-RjcYGu
📚 𝐂𝐨𝐮𝐫𝐬𝐞: NLP & LLM Course by Cohere https://lnkd.in/eKys6E5U
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Fine-tuning Large Language Models (short): https://lnkd.in/ehVRv-ZE
llms
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: Improving Language Understanding by Generative Pre-Training https://lnkd.in/eg9FDgXq
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: #gpt3 Language Models are Few-Shot Learners https://lnkd.in/e7CN3J-t

📌 𝐑𝐀𝐆 𝐒𝐲𝐬𝐭𝐞𝐦𝐬
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Semantic Search : https://lnkd.in/eRak73U9
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Building Applications with Vector Databases (short) by DeepLearning.AI https://lnkd.in/etxjqdJs
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: #RAG for Knowledge-Intensive NLP Tasks https://lnkd.in/eGwmUbHs

📌 𝐆𝐞𝐧 𝐀𝐈
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Generative Path by Google: https://lnkd.in/eyms3zrM
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Full-stack LLM https://lnkd.in/eEvnJ3W7
📚 𝐂𝐨𝐮𝐫𝐬𝐞: Deploying GPT and Pre-trained Models https://lnkd.in/eXfhFHnj
📚 𝐂𝐨𝐮𝐫𝐬𝐞: RLHF (short): https://lnkd.in/e4bG_k6J
📑 𝐀𝐫𝐭𝐢𝐜𝐥𝐞: Prompt Engineering basics: https://lnkd.in/eYK_fWQV


📌 𝐋𝐋𝐌𝐎𝐏𝐬
📚 𝐂𝐨𝐮𝐫𝐬𝐞: LLMOps (short) https://lnkd.in/e7nT4A_2
📚 𝐂𝐨𝐮𝐫𝐬𝐞 Functions, Tools and Agents with LangChain (short) https://lnkd.in/eEWvf8-d
📚 𝐂𝐨𝐮𝐫𝐬𝐞 : Automated testing LLMOPs (short) https://lnkd.in/e6ZCeytC

### [To improve RAG](https://www.linkedin.com/feed/update/urn:li:activity:7167030145327779840?updateEntityUrn=urn%3Ali%3Afs_updateV2%3A%28urn%3Ali%3Aactivity%3A7167030145327779840%2CFEED_DETAIL%2CEMPTY%2CDEFAULT%2Cfalse%29)

## Hugging Face NLP Course
- [Hugging Face NLP Course on Udemy](https://www.udemy.com/home/my-courses/learning/)

## Transformers summary

[GPT2 Explanation b Jalmar](https://jalammar.github.io/illustrated-gpt2/)

## [LLM Finetuning Notebooks ](https://github.com/ashishpatel26/LLM-Finetuning)

## [Langchain Handbook](https://www.pinecone.io/learn/series/langchain/)
## [Pineconde Vector Database Manulas](https://www.pinecone.io/learn/#missing-manuals)
## [Langchain Handbook](https://github.com/pinecone-io/examples/tree/master/learn/generation/langchain/handbook)
## [Retrieval Augmented Generation](https://aman.ai/primers/ai/RAG/)

An unmissable LLM playlist that encompasses the latest LLama2, Falcon finetuning, to advanced techniques such as LORA, Flash attention, Multiquery attention, etc and also delves into the intricate workings of chat-Gpt like RLHF etc.

🌟 The Making of SRKGPT: Crafting an AI with Shahrukh Khan's Style | LLM | LLama v2 Finetuning youtu.be/gYPwx0DR7zc 
🌟 Supercharging LLama-2: Enhancing Performance on Any Task with ChatGPT Dataset youtu.be/paGr-t1wSOQ
🌟 Elevating Base Falcon Model with ChatGPT Dataset: A Game-Changing Approach youtu.be/lo11Iczb0Vc
🌟 LLAMA-2 Open-Source LLM: Custom Fine-tuning Made Easy on a Single-GPU Colab Instance | PEFT | LORA youtu.be/8cc4bJtycOA
🌟 Falcon Open-Source LLM: Custom Fine-tuning Made Easy on a Single-GPU Colab Instance | PEFT | LORA youtu.be/CxqZ5j3xlt0
🌟 Conversational AI : Understanding the Technology behind Chat-GPT | GPT| RLHF | Few Shot Inferences
youtu.be/JKoJ5YIr2O4
🌟 The Good, the Bad, & the Ugly : Exploring the Dual Nature of Conversational LLMs | Open-Source LLMs
youtu.be/MHfzoHC4kek
🌟 Pushing the Boundaries of LLMs: Sparse & Flash Attention, Quantisation, Pruning, Distillation, LORA
youtu.be/mF7OM_XU2S4

📌Hands-On LLMs (Pau Labarta Bajo, Paul Iusztin & and Alexandru Razvant are also my 3 favourite creators on Linkedin!): An excellent course that focuses on the practical application of LLMs. Not just notebooks, but an actual product! 
Link: https://lnkd.in/gepah3Nz
📌Activeloop & Towards AI LLMs Course: Covers theory about the history of LLMs and practical use cases!
Link: https://lnkd.in/gmxQ4MC8
📌LLM Courses by Weights & Biases: Excellent courses for learning the basics of LLMs and specific modules on LLMOps included. 
Link: https://lnkd.in/gmxQ4MC8

## [Comprehensive list of Tutorials and courses on LangChain](https://python.langchain.com/docs/additional_resources/tutorials)


**I. Introduction to Transformer Architecture**
   - A. Language Model Evolution
   - B. Power of Transformer
      1. Learning Relevance
      2. Context Understanding
   - C. Self-Attention Significance
      - i. Visualization
      - ii. Language Encoding

**II. Transformer Architecture Overview**
   - A. Encoder-Decoder Components
   - B. Shared Characteristics
   - C. Diagram Structure
      - i. Input-Output Alignment

**III. Tokenization and Embedding**
   - A. Word to Number Conversion
   - B. High-Dimensional Vector Space
      - i. Preserving Context
      - ii. Word2Vec Roots

**IV. Positional Encoding**
   - A. Maintaining Word Order
   - B. Parallel Token Processing

**V. Self-Attention Mechanism**
   - A. Analyzing Token Relationships
      - i. Attention Weights
      - ii. Multi-Headed Self-Attention
         - a. Diverse Language Aspects
      - iii. Learning Linguistic Features

**VI. Output Generation**
   - A. Feed-Forward Network
   - B. Logits Generation
   - C. Softmax Layer
      - i. Probability Distribution
   - D. Final Token Selection

What is a 𝗩𝗲𝗰𝘁𝗼𝗿 𝗗𝗮𝘁𝗮𝗯𝗮𝘀𝗲?

With the rise of Foundational Models, Vector Databases skyrocketed in popularity. The truth is that a Vector Database is also useful outside of a Large Language Model context.

When it comes to Machine Learning, we often deal with Vector Embeddings. Vector Databases were created to perform specifically well when working with them:

➡️ Storing.
➡️ Updating.
➡️ Retrieving.

When we talk about retrieval, we refer to retrieving set of vectors that are most similar to a query in a form of a vector that is embedded in the same Latent space. This retrieval procedure is called Approximate Nearest Neighbour (ANN) search.

A query here could be in a form of an object like an image for which we would like to find similar images. Or it could be a question for which we want to retrieve relevant context that could later be transformed into an answer via a LLM.

Let’s look into how one would interact with a Vector Database:

𝗪𝗿𝗶𝘁𝗶𝗻𝗴/𝗨𝗽𝗱𝗮𝘁𝗶𝗻𝗴 𝗗𝗮𝘁𝗮.

1. Choose a ML model to be used to generate Vector Embeddings.
2. Embed any type of information: text, images, audio, tabular. Choice of ML model used for embedding will depend on the type of data.
3. Get a Vector representation of your data by running it through the Embedding Model.
4. Store additional metadata together with the Vector Embedding. This data would later be used to pre-filter or post-filter ANN search results.
5. Vector DB indexes Vector Embedding and metadata separately. There are multiple methods that can be used for creating vector indexes, some of them: Random Projection, Product Quantization, Locality-sensitive Hashing.
6. Vector data is stored together with indexes for Vector Embeddings and metadata connected to the Embedded objects.

𝗥𝗲𝗮𝗱𝗶𝗻𝗴 𝗗𝗮𝘁𝗮.

7. A query to be executed against a Vector Database will usually consist of two parts:

➡️ Data that will be used for ANN search. e.g. an image for which you want to find similar ones.
➡️ Metadata query to exclude Vectors that hold specific qualities known beforehand. E.g. given that you are looking for similar images of apartments - exclude apartments in a specific location.

8. You execute Metadata Query against the metadata index. It could be done before or after the ANN search procedure.
9. You embed the data into the Latent space with the same model that was used for writing the data to the Vector DB.
10. ANN search procedure is applied and a set of Vector embeddings are retrieved. Popular similarity measures for ANN search include: Cosine Similarity, Euclidean Distance, Dot Product.

Some popular Vector Databases: Qdrant, Pinecone, Weviate, Milvus, Faiss, Vespa.
![Vector Database](Vector_databse.gif)

This revised version simplifies the explanation, breaking down the content into clear sections. The introduction sets the stage, and subsequent sections provide a step-by-step understanding of the transformer architecture, from its components to the generation of output. Each subheading encapsulates a specific aspect, ensuring a concise and logical progression through the topic.

The introduction emphasizes the transformative impact of the transformer architecture on natural language processing, surpassing the limitations of earlier models like RNNs. The core strength lies in the ability to grasp word relevance and context within a sentence, not just neighboring words but all words. The concept of self-attention is pivotal, allowing the model to discern relationships between words irrespective of their positions. The attention map visualizes this, demonstrating how certain words, like "book," strongly connect with others, enhancing the model's language encoding capabilities.

The subsequent section delves into the transformer architecture, presenting a simplified diagram of the encoder and decoder components. These components work collaboratively, with both sharing similarities. Tokenization, the process of converting words into numerical representations, is essential before inputting them into the model. The embedding layer then maps tokenized words to vectors within a high-dimensional space, preserving contextual meaning. This embedding space concept has roots in previous algorithms like Word2Vec. The size of these vectors, exemplified as three-dimensional for simplicity, encodes relationships and meaning in the input sequence.

Positional encoding is introduced to maintain word order relevance during parallel processing of input tokens in the encoder or decoder. The subsequent self-attention layer analyzes relationships between tokens, employing multi-headed self-attention for diverse language aspects. Each attention head learns different linguistic features, and their combination contributes to a comprehensive understanding of language.

The attention weights, learned during training, guide the model's focus on relevant parts of the input sequence. The process is not a one-time affair; multi-headed self-attention ensures a nuanced understanding of various linguistic nuances. Following attention weights application, the output passes through a feed-forward network, producing logits proportional to the probability scores for each token in the vocabulary. A softmax layer normalizes these scores, generating a probability distribution for each word. The most likely predicted token is the one with the highest probability. The discussion hints at the versatility of final token selection, a concept to be explored in later course modules.


Certainly, let's break down the explanation into more concise sections, highlighting key subheadings and topics:

**Introduction to Transformer Architecture:**
The introduction underscores the transformative impact of the transformer architecture, surpassing RNNs. The self-attention mechanism allows the model to understand word relevance and context, a key feature demonstrated through attention maps.

**Overview of Transformer Architecture:**
Presenting a simplified diagram, the architecture is split into encoder and decoder components, working collaboratively. Tokenization converts words to numerical representations, and the embedding layer maps these tokens to high-dimensional vectors, encoding contextual meaning.

**Positional Encoding and Parallel Processing:**
To maintain word order relevance during parallel processing, positional encoding is introduced. This ensures the model doesn't lose the significance of word positions in the sentence.

**Self-Attention Mechanism:**
The self-attention layer analyzes relationships between input tokens, crucial for capturing contextual dependencies. Multi-headed self-attention further enhances the model's ability to understand various linguistic nuances.

**Attention Weights and Multi-Headed Self-Attention:**
Attention weights, learned during training, guide the model's focus on relevant parts of the input sequence. Multi-headed self-attention introduces diversity, allowing different heads to focus on distinct linguistic features.

**Feed-Forward Network and Logits:**
The output of self-attention is processed through a feed-forward network, producing logits proportional to the probability scores for each token. This output undergoes normalization through a softmax layer, resulting in a probability distribution for each word.

**Token Selection and Final Output:**
The most likely predicted token is determined by the one with the highest probability. The explanation hints at the flexibility of final token selection, a concept to be explored in later course modules.

This breakdown provides a more concise overview of each section, emphasizing the key subheadings and topics within the explanation of the transformer architecture.
Building large language models using the transformer architecture dramatically improved the performance of natural language tasks over the earlier generation of RNNs, and led to an explosion in regenerative capability. The power of the transformer architecture lies in its ability to learn the relevance and context of all of the words in a sentence. Not just as you see here, to each word next to its neighbor, but to every other word in a sentence. To apply attention weights to those relationships so that the model learns the relevance of each word to each other words no matter where they are in the input. This gives the algorithm the ability to learn who has the book, who could have the book, and if it's even relevant to the wider context of the document. These attention weights are learned during LLM training and you'll learn more about this later this week. This diagram is called an attention map and can be useful to illustrate the attention weights between each word and every other word. Here in this stylized example, you can see that the word book is strongly connected with or paying attention to the word teacher and the word student. This is called self-attention and the ability to learn a tension in this way across the whole input significantly approves the model's ability to encode language. Now that you've seen one of the key attributes of the transformer architecture, self-attention, let's cover at a high level how the model works. Here's a simplified diagram of the transformer architecture so that you can focus at a high level on where these processes are taking place. The transformer architecture is split into two distinct parts, the encoder and the decoder. These components work in conjunction with each other and they share a number of similarities. Also, note here, the diagram you see is derived from the original attention is all you need paper. Notice how the inputs to the model are at the bottom and the outputs are at the top, where possible we'll try to remain faithful to this throughout the course. Now, machine-learning models are just big statistical calculators and they work with numbers, not words. So before passing texts into the model to process, you must first tokenize the words. Simply put, this converts the words into numbers, with each number representing a position in a dictionary of all the possible words that the model can work with. You can choose from multiple tokenization methods. For example, token IDs matching two complete words, or using token IDs to represent parts of words. As you can see here. What's important is that once you've selected a tokenizer to train the model, you must use the same tokenizer when you generate text. Now that your input is represented as numbers, you can pass it to the embedding layer. This layer is a trainable vector embedding space, a high-dimensional space where each token is represented as a vector and occupies a unique location within that space. Each token ID in the vocabulary is matched to a multi-dimensional vector, and the intuition is that these vectors learn to encode the meaning and context of individual tokens in the input sequence. Embedding vector spaces have been used in natural language processing for some time, previous generation language algorithms like Word2vec use this concept. Don't worry if you're not familiar with this. You'll see examples of this throughout the course, and there are some links to additional resources in the reading exercises at the end of this week. Looking back at the sample sequence, you can see that in this simple case, each word has been matched to a token ID, and each token is mapped into a vector. In the original transformer paper, the vector size was actually 512, so much bigger than we can fit onto this image. For simplicity, if you imagine a vector size of just three, you could plot the words into a three-dimensional space and see the relationships between those words. You can see now how you can relate words that are located close to each other in the embedding space, and how you can calculate the distance between the words as an angle, which gives the model the ability to mathematically understand language. As you add the token vectors into the base of the encoder or the decoder, you also add positional encoding. The model processes each of the input tokens in parallel. So by adding the positional encoding, you preserve the information about the word order and don't lose the relevance of the position of the word in the sentence. Once you've summed the input tokens and the positional encodings, you pass the resulting vectors to the self-attention layer. Here, the model analyzes the relationships between the tokens in your input sequence. As you saw earlier, this allows the model to attend to different parts of the input sequence to better capture the contextual dependencies between the words. The self-attention weights that are learned during training and stored in these layers reflect the importance of each word in that input sequence to all other words in the sequence. But this does not happen just once, the transformer architecture actually has multi-headed self-attention. This means that multiple sets of self-attention weights or heads are learned in parallel independently of each other. The number of attention heads included in the attention layer varies from model to model, but numbers in the range of 12-100 are common. The intuition here is that each self-attention head will learn a different aspect of language. For example, one head may see the relationship between the people entities in our sentence. Whilst another head may focus on the activity of the sentence. Whilst yet another head may focus on some other properties such as if the words rhyme. It's important to note that you don't dictate ahead of time what aspects of language the attention heads will learn. The weights of each head are randomly initialized and given sufficient training data and time, each will learn different aspects of language. While some attention maps are easy to interpret, like the examples discussed here, others may not be. Now that all of the attention weights have been applied to your input data, the output is processed through a fully-connected feed-forward network. The output of this layer is a vector of logits proportional to the probability score for each and every token in the tokenizer dictionary. You can then pass these logits to a final softmax layer, where they are normalized into a probability score for each word. This output includes a probability for every single word in the vocabulary, so there's likely to be thousands of scores here. One single token will have a score higher than the rest. This is the most likely predicted token. But as you'll see later in the course, there are a number of methods that you can use to vary the final selection from this vector of probabilities.


### Scaling Laws
Scaling laws refer to the relationship between the model's performance and factors such as the number of parameters, the size of the training dataset, the compute budget, and the network architecture. They were discovered after a lot of experiments and are described in the Chinchilla paper. These laws provide insights into how to allocate resources when training these models optimally.

The main elements characterizing a language model are:

- The number of parameters (N) reflects the model's capacity to learn from data. More parameters allow the model to capture complex patterns in the data.
- The size of the training dataset (D) is measured in the number of tokens (small pieces of text ranging from a few words to a single character).
- FLOPs (floating point operations per second) measure the compute budget used for training.

### Hallucinations and Biases in LLMs
The term hallucinations refers to instances where AI systems generate outputs, such as text or images, that don't align with real-world facts or inputs. For example, ChatGPT might generate a plausible-sounding answer to an entirely incorrect factual question. 

Hallucinations in LLMs refer to instances where the model generates outputs that do not align with real-world facts or context. This can lead to the propagation of misinformation, especially in critical sectors like healthcare and education where the accuracy of information is of utmost importance. Similarly, bias in LLMs can result in outputs that favor certain perspectives over others, potentially leading to the reinforcement of harmful stereotypes and discrimination.
Consider an interaction where a user asks, "Who won the World Series in 2025?" If the LLM responds with, "The New York Yankees won the World Series in 2025," it's a clear case of hallucination. As of now (July 2023), the 2025 World Series hasn't taken place, so any claim about its outcome is a fabrication.

Bias in AI and LLMs is another significant issue. It refers to these models' inclination to favor specific outputs or decisions based on their training data. If the training data is predominantly from a specific region, the model might show a bias toward that region's language, culture, or perspectives. If the training data contains inherent biases, such as gender or racial bias, the AI system might produce skewed or discriminatory outputs.

For example, if a user asks an LLM, "Who is a nurse?" and it responds with, "She is a healthcare professional who cares for patients in a hospital,” it shows a gender bias. The model automatically associates nursing with women, which doesn't accurately reflect the reality where both men and women can be nurses.

Mitigating hallucinations and bias in AI systems involves refining model training, using verification techniques, and ensuring the training data is diverse and representative. Finding a balance between maximizing the model's potential and avoiding these issues remains challenging.

Interestingly, in creative domains like media and fiction writing, these "hallucinations" can be beneficial, enabling the generation of unique and innovative content.
