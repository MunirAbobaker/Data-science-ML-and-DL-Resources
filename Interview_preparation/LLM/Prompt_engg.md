## Table of Contents
- [4 Main LLM Limitations](#4-main-llm-limitations)
- [LLM Parameters](#llm-parameters)
- [Temperature](#temperature)
- [Top_p](#top_p)
- [What is a System Prompt?](#what-is-a-system-prompt)
- [System Prompts in Real-World Scenarios](#system-prompts-in-real-world-scenarios)
- [Terminology](#terminology)
- [Prompt Engineering](#prompt-engineering)
- [In-Context Learning](#in-context-learning)
- [Zero-Shot Inference](#zero-shot-inference)
- [One-Shot Inference](#one-shot-inference)
- [Few-Shot Inference](#few-shot-inference)
- [Context Window Limitations](#context-window-limitations)
- [Fine-Tuning](#fine-tuning)
- [Model Scale and Task Performance](#model-scale-and-task-performance)
- [Experimenting with Models](#experimenting-with-models)
- [Configuration Settings](#configuration-settings)
- [Future Exploration](#future-exploration)
- [Language Model Configuration Guide](#language-model-configuration-guide)
  - [Overview](#overview)
  - [Configuration Parameters](#configuration-parameters)
    - [Max New Tokens](#max-new-tokens)
    - [Softmax Layer](#softmax-layer)
  - [Decoding Techniques](#decoding-techniques)
    - [Greedy Decoding](#greedy-decoding)
    - [Random Sampling](#random-sampling)
    - [Top k and Top p Sampling](#top-k-and-top-p-sampling)
    - [Temperature](#temperature-1)
  - [Implementation Tips](#implementation-tips)
  - [Example](#example)
  - [Conclusion](#conclusion)

## 4 Main LLM Limitations
1. Based on trained data
2. Token limits
3. Hallucinations
4. Connecting with Third Party Services

## LLM Parameters
- [LLM Parameters](https://datasciencedojo.com/blog/llm-parameters/)
- [Refer This](https://txt.cohere.com/llm-parameters-best-outputs-language-ai/)

### Temperature
This parameter controls the randomness of the AI’s output.

### Top_p
Also known as ‘nucleus sampling‘.

### What is a System Prompt?
A system prompt is your introductory overview that sets the tone and provides the general context for the task.

### System Prompts in Real-World Scenarios
System prompts can be super handy in various real-world scenarios.

## Terminology
- **Prompt:** Input text given to the model.
- **Inference:** Process of generating text by the model.
- **Completion:** Output text generated by the model.
- **Context Window:** Full text or memory available for the prompt.

## Prompt Engineering
- [Prompt Engineering Guide](https://www.promptingguide.ai/)
- Prompting Techniques
- Prompt Engineering helps to effectively design and improve prompts to get better results on different tasks with LLMs.
- While the previous basic examples were fun, in this section we cover more advanced prompting engineering techniques that allow us to achieve more complex tasks and improve reliability and performance of LLMs.
- Zero-shot Prompting
- Few-shot Prompting
- Chain-of-Thought Prompting
- Self-Consistency
- Generate Knowledge Prompting
- Prompt Chaining
- Tree of Thoughts
- Retrieval Augmented Generation
- Automatic Reasoning and Tool-use
- Automatic Prompt Engineer
- Active-Prompt
- Directional Stimulus Prompting
- Program-Aided Language Models
- ReAct
- Reflexion
- Multimodal CoT
- Graph Prompting

## How to tune [LLM Parameters](https://www.linkedin.com/pulse/7-key-llm-parameters-everyone-designing-prompts-should-kimothi-rjcmc/) for optimal performance
- Shape your model performance using LLM parameters. Imagine you have a super-smart computer program. You type something into it, like a question or a sentence, and you want it to guess what words should come next. This program doesn’t just guess randomly; it’s like a detective that looks at all the possibilities and says, “Hmm, these words are more likely to come next.”
- It makes an extensive list of words and says, “Here are all the possible words that could come next, and here’s how likely each one is.” But here’s the catch: it only gives you one word, and that word depends on how you tell the program to make its guess. You set the rules, and the program follows them.
- So, it’s like asking your computer buddy to finish your sentences, but it’s super smart and calculates the odds of each word being the right fit based on what you’ve typed before.
- That’s how this model works, like a word-guessing detective, giving you one word based on how you want it to guess.
- A brief introduction to Large Language Model parameters
- Large language model parameters refer to the configuration settings and components that define the behavior of a large language model (LLM), which is a type of artificial intelligence model used for natural language processing tasks.
- How do LLM parameters work
- LLM parameters include the architecture, model size, training data, and hyperparameters. The core component is the transformer architecture, which enables LLMs to process and generate text efficiently. LLMs are trained in vast datasets, learning patterns and relationships between words and phrases.
- They use vectors to represent words numerically, allowing them to understand and generate text. During training, these models adjust their parameters (weights and biases) to minimize the difference between their predictions and the actual data. Let’s have a look at the key parameters in detail.
- Learn in detail about fine-tuning LLMs 
- 1. Model:
- The model size refers to the number of parameters in the LLM. A parameter is a variable that is learned by the LLM during training. The model size is typically measured in billions or trillions of parameters. A larger model size will typically result in better performance, but it will also require more computing resources to train and run.
- Also, it is a specific instance of an LLM trained on a corpus of text. Different models have varying sizes and are suitable for different tasks. For example, GPT-3 is a large model with 175 billion parameters, making it highly capable in various natural language understanding and generation tasks.
- 2. Number of tokens:
- The number of tokens refers to the size of the vocabulary that the LLM is trained on. A token is a unit of text, such as a word, a punctuation mark, or a number. The number of tokens in a vocabulary can vary greatly, from a few thousand to several million. A larger vocabulary allows the LLM to generate more creative and accurate text, but it also requires more computing resources to train and run.
- The number of tokens in an LLM’s vocabulary impacts its language understanding. For instance, GPT-2 has a vocabulary size of 1.5 billion tokens. Larger vocabulary allows the model to comprehend a wider range of words and phrases.
- Fully Functional Chatbot with Llama Index: Build a Custom ChatGPT
- 3. Temperature:
- The temperature is a parameter that controls the randomness of the LLM’s output. A higher temperature will result in more creative and imaginative text, while a lower temperature will result in more accurate and factual text.
- For example, if you set the temperature to 1.0, the LLM will always generate the most likely next word. However, if you set the temperature to 2.0, the LLM will be more likely to generate less likely next words, which could result in more creative text.
- 4. Context window:
- The context window is the number of words that the LLM considers when generating text. A larger context window will allow the LLM to generate more contextually relevant text, but it will also make the training process more computationally expensive. For example, if the context window is set to 2, the LLM will consider the two words before and after the current word when generating the next word.
- The context window determines how far back in the text the model looks when generating responses. A longer context window enhances coherence in conversation, crucial for chatbots.
- For example, when generating a story, a context window of 1024 tokens can ensure consistency and context preservation.
- Learn about Build custom LLM applications
- 5. Top-k and Top-p:
- These techniques filter token selection. Top-k selects the top-k most likely tokens, ensuring high-quality output. Top-p, on the other hand, sets a cumulative probability threshold, retaining tokens with a total probability above it. Top-k is useful for avoiding nonsensical responses, while Top-p can ensure diversity.
- For example, if you set Top-k to 10, the LLM will only consider the 10 most probable next words. This will result in more fluent text, but it will also reduce the diversity of the text. If you set Top-p to 0.9, the LLM will only generate words that have a probability of at least 0.9. This will result in more diverse text, but it could also result in less fluent text.
- 6. Stop sequences:
- LLMs can be programmed to avoid generating specific sequences, such as profanity or sensitive information. For example, a content moderation system can use stop sequences to prevent the model from generating harmful content.
- For example, you could add the stop sequence “spam” to the LLM, so that it would never generate the word “spam”.
- 7. Frequency and presence penalties:
- Frequency Penalty penalizes the LLM for generating words that are frequently used. This can be useful for preventing the LLM from generating repetitive text. Presence Penalty penalizes the LLM for generating words that have not been used recently. This can be useful for preventing the LLM from generating irrelevant text.
- These penalties influence token generation. A presence penalty discourages the use of specific tokens, while a frequency penalty encourages token use. For instance, in language translation, a frequency penalty can be applied to ensure that rare words are used more often.
- LLM parameters example
- Consider a chatbot using GPT-3 (model). To maintain coherent conversations, it uses a longer context window (context window). To avoid inappropriate responses, it employs stop sequences to filter out offensive content (stop sequences). Temperature is set lower to provide precise, on-topic answers, and Top-k ensures the best token selection for each response (temperature, Top-k).
- These parameters enable fine-tuning of LLM behavior, making them adaptable to diverse applications, from chatbots to content generation and translation.
- Shape the capabilities of LLMs
- LLMs have diverse applications, such as chatbots (e.g., ChatGPT), language translation, text generation, sentiment analysis, and more. They can generate human-like text, answer questions, and perform various language-related tasks. LLMs have found use in automating customer support, content creation, language translation, and data analysis, among other fields.
- For example, in customer support, LLMs can provide instant responses to user queries, improving efficiency. In content creation, they can generate articles, reports, and even code snippets based on provided prompts. In language translation, LLMs can translate text between languages with high accuracy.
- In summary, large language model parameters are essential for shaping the capabilities and behavior of LLMs, making them powerful tools for a wide range of natural language processing tasks.
