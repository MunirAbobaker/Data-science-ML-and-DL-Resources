# Data-science-ML-and-DL-Resources
To learn ML,DL resources

## Python

![Python-Symbol](https://user-images.githubusercontent.com/110838853/226786519-b758e13b-0a29-4d72-8189-2fd84bfd7d2c.png)

### Python-Roadmap 
https://github.com/ashishpatel26/Python-Roadmap

### Python Code Tutorials For all 
https://github.com/ashishpatel26/pythoncode-tutorials ### here We can Find python code for different kind of Projects for CNN,NLP,web Programming and ML

##  Data science
#### Data science Roadmap for ML,DL,NLP,python,CNN,MLops
https://github.com/hemansnation/God-Level-Data-Science-ML-Full-Stack

## Artificial Intelligence 
Artificial intelligence (AI) is a broad field of computer science and engineering that focuses on developing intelligent machines that can perform tasks that typically require human intelligence, such as perception, reasoning, learning, and problem-solving.

AI systems are designed to learn from and adapt to their environment, enabling them to improve their performance over time. These systems can be classified into several categories based on their level of intelligence:

Reactive machines: These AI systems can only react to specific input data without having any memory or ability to learn from previous experiences. Examples of reactive machines include Deep Blue, a chess-playing computer program, and Siri, an intelligent personal assistant.

Limited memory machines: These AI systems can store and recall previous experiences, allowing them to make more informed decisions. Examples of limited memory machines include self-driving cars and personal robots.

Theory of mind machines: These AI systems have the ability to understand and predict the behavior of others by modeling their beliefs, desires, and intentions. Theory of mind machines are still largely theoretical, but they are an active area of research in AI.

Self-aware machines: These AI systems have a sense of self and consciousness, allowing them to understand their own state and emotions. Self-aware machines are still purely theoretical and may never be possible to create.

AI has many practical applications, including natural language processing, image and speech recognition, predictive analytics, and robotics. It is a rapidly advancing field, with new techniques and applications being developed every day.


![AI](https://user-images.githubusercontent.com/110838853/226786219-d42789f0-49d8-4656-83bf-83e7e7b72b01.jpeg)

#### Artificial Intelligence for Beginners - A Curriculum 
https://github.com/ashishpatel26/AI-For-Beginners

#### Artificial Intelligence Resources 
https://github.com/ahmedbahaaeldin/From-0-to-Research-Scientist-resources-guide  ##Detailed and tailored guide for undergraduate students or anybody want to dig deep into the field of AI with solid foundation.


![AUG-2022_AI-Learning-fig1](https://user-images.githubusercontent.com/110838853/226782113-03fc1525-fb6d-47ce-9cda-4c534f12a27b.jpg)

## Mathematics for ML
#### A collection of resources to learn mathematics for machine learning
https://github.com/dair-ai/Mathematics-for-ML

## Machine learning
Machine learning (ML) is a subfield of artificial intelligence (AI) that involves training algorithms to automatically learn patterns from data, without being explicitly programmed.

In machine learning, algorithms are designed to learn from data, identify patterns, and make predictions or decisions based on that data. The algorithms are trained using a variety of techniques such as supervised, unsupervised, and reinforcement learning.

Supervised learning involves training an algorithm on labeled data, where the input and output pairs are known. The algorithm learns to map inputs to outputs, allowing it to make predictions on new, unseen data. Examples of supervised learning include image classification, sentiment analysis, and spam detection.

Unsupervised learning involves training an algorithm on unlabeled data, where the algorithm must discover patterns or structure within the data. Clustering, anomaly detection, and dimensionality reduction are examples of unsupervised learning.

Reinforcement learning involves training an agent to make decisions in an environment by rewarding or punishing it based on its actions. The agent learns to maximize its reward over time, allowing it to make better decisions in the future.

Machine learning has many practical applications, including natural language processing, image and speech recognition, predictive analytics, and recommendation systems. It has become an essential tool for many businesses and organizations seeking to make data-driven decisions and automate complex tasks

![ml-e1610553826718](https://user-images.githubusercontent.com/110838853/226786032-2162bb7b-0903-4ed1-b468-97c257bc51ce.jpg)


#### Andrew NG Notes on ML
https://github.com/ashishpatel26/Andrew-NG-Notes

#### ML Course Notes and Videos
https://github.com/ashishpatel26/ML-Course-Notes


#### ML,DL,NLP Youtube Courses 
https://github.com/dair-ai/ML-YouTube-Courses


### Stanford Course(Standford CS231n 2017 Summary)
https://github.com/mbadry1/CS231n-2017-Summary

#### Data-Science-Tutorial-By-Lambda-School
https://github.com/ashishpatel26/Data-Science-Tutorial-By-Lambda-School

#### Explanation to key concepts in ML
https://github.com/dair-ai/ML-Papers-Explained

### ML Notebooks 
https://github.com/dair-ai/Mathematics-for-ML

#### Machine Learning Interview Preparation
https://www.geeksforgeeks.org/machine-learning/


## Deep learning
Deep learning is a subfield of machine learning that is based on artificial neural networks with many layers. These networks are designed to learn complex representations of data through the use of multiple layers of interconnected nodes, each of which performs a simple computation.

In deep learning, the algorithm learns to extract features from raw data, such as images, sounds, and text, without the need for human feature engineering. This is achieved by stacking multiple layers of non-linear processing units, also known as neurons, to form a neural network that can automatically learn the features and relationships between them.

Deep learning models have shown impressive results in a variety of applications, including image and speech recognition, natural language processing, and game playing. For example, convolutional neural networks (CNNs) have been used to achieve state-of-the-art results in image classification, while recurrent neural networks (RNNs) have been used for speech recognition and natural language processing.

One of the key advantages of deep learning is its ability to learn from large amounts of data, making it particularly effective in applications where large datasets are available. Additionally, deep learning models can automatically learn to extract features from raw data, eliminating the need for manual feature engineering, which can be a time-consuming and error-prone process.

Overall, deep learning has become an essential tool in many areas of research and industry, enabling breakthroughs in fields such as computer vision, natural language processing, and robotics.
![ezgif-3-091b9a24fd](https://user-images.githubusercontent.com/110838853/226787169-b81940d2-399b-4038-ae19-9bf3ad530bbe.jpg)


### Deeplearning.ai Course 

https://github.com/mbadry1/DeepLearning.ai-Summary

https://github.com/mbadry1/DeepLearning.ai-Summary/tree/master/1-%20Neural%20Networks%20and%20Deep%20Learning


https://github.com/Avik-Jain/DeepLearning.ai-Summary/tree/master/1-%20Neural%20Networks%20and%20Deep%20Learning

#### Andrew NG Notes on ML
https://github.com/ashishpatel26/Andrew-NG-Notes

#### ML,DL,NLP Youtube Courses 
https://github.com/dair-ai/ML-YouTube-Courses


#### Deep learning Course Notes and Videos
https://github.com/ashishpatel26/ML-Course-Notes

### Practical Deep learning for Coders from fast.ai-
https://course.fast.ai/Lessons/lesson1.html


### Full Stack Deep learning
https://fall2019.fullstackdeeplearning.com/course-content/setting-up-machine-learning-projects/metrics


### Deep learning Book
https://github.com/janishar/mit-deep-learning-book-pdf

### Deep Learning Notebooks 
https://github.com/dair-ai/Mathematics-for-ML




###  Trending-Deep-Learning
https://github.com/mbadry1/Trending-Deep-Learning
https://github.com/mbadry1/Top-Deep-Learning ##  Top Deep learning Repositories

### Data science process
https://resources.experfy.com/bigdata-cloud/the-data-science-process/

### How to build a regression-model
https://resources.experfy.com/bigdata-cloud/how-to-build-a-regression-model-in-python/

### How to build a model
https://resources.experfy.com/ai-ml/how-to-build-a-machine-learning-model/

https://copyassignment.com/8-steps-to-build-a-machine-learning-model/



## Neural network

![1_3fA77_mLNiJTSgZFhYnU0Q](https://user-images.githubusercontent.com/110838853/226782532-c79583a7-fe4e-4af8-92be-6c19dc261db4.png)

One min to have a general idea about Neural Network, which is a technique that could achieve many different aims, facial recognition, predicting, music composition, etc.

NN contains virtual ‘neurons’ that are arranged in layers that are connected to each other. The neurons pass on the information and thereby perform calculations.

The connections between the neurons also have values associated with them, called weights.

Those weights tell us how much the information from one layer matters for the next layer. The values of neurons and the weights of the connections are essentially the free parameters of the network.

By training the network, we want to find those values of the parameters that minimize a certain function, called the ‘loss function’. It is an optimization problem, we use a method called backpropagation to solve it.

Backpropagation means if the NN gives us a result that is not good enough, we go back and change the weights of neurons and their connections. This is how the network can learn from failure.
![1645940317303](https://user-images.githubusercontent.com/110838853/224456961-62efe7a4-e1af-4002-89e2-0de91f987538.gif)


#### Tools to Design or Visualize Architecture of Neural Network
https://github.com/ashishpatel26/Tools-to-Design-or-Visualize-Architecture-of-Neural-Network

## Tensorflow
TensorFlow is an open-source software library for dataflow and differentiable programming across a range of tasks. It was developed by the Google Brain team and released in 2015.

TensorFlow is designed to enable efficient numerical computation using data flow graphs. In a data flow graph, nodes represent mathematical operations, while edges represent the data that flows between them. This allows complex mathematical computations to be expressed in a simple, intuitive way, making it easier to develop and train machine learning models.

TensorFlow provides a comprehensive ecosystem of tools and libraries for building and deploying machine learning models, including:

TensorFlow Core: The central component of the TensorFlow library, providing support for building and training machine learning models using data flow graphs.

Keras: A high-level neural networks API that makes it easy to build and train deep learning models.

TensorFlow Extended (TFX): A platform for building end-to-end machine learning pipelines, from data ingestion to model serving.

TensorFlow.js: A JavaScript library for training and deploying machine learning models in the browser or on Node.js.

TensorFlow Lite: A lightweight version of TensorFlow designed for mobile and embedded devices.

Overall, TensorFlow has become a widely used tool in the machine learning community, enabling researchers and developers to build and deploy state-of-the-art machine learning models for a wide range of applications.

![model](https://user-images.githubusercontent.com/110838853/226787695-e9a735a2-c3f0-4e71-bf69-a270b8f5efa1.png)

#### Learning Tensorflow Step by Step:: Concepts, Examples & Applications
https://github.com/suvoooo/Learn-TensorFlow


#### Introduction to Tensorflow 2 for Computer Vision by AIFEE -
https://learn.aifee.co/p/introduction-to-tensorflow-2-for-computer-vision?ck_subscriber_id=2076754537&utm_source=convertkit&utm_medium=email&utm_campaign=Welcome+to+AIFEE+Newsletter%21%20-%204653438

#### Tensorflow Models
https://github.com/tensorflow/models/tree/master/research  ##Models and examples built with TensorFlow

## Pytorch

PyTorch is an open-source machine learning framework that was developed by Facebook AI Research (FAIR) and released in 2016. It is built on top of the Python programming language and is designed to provide a flexible and easy-to-use platform for building and training machine learning models.

PyTorch uses a dynamic computational graph, which means that the graph is generated on-the-fly during the execution of the program. This provides a high degree of flexibility and allows for efficient implementation of complex models, such as recurrent neural networks and generative adversarial networks.

PyTorch also provides a range of tools and libraries for building and training machine learning models, including:

TorchScript: A tool for serializing PyTorch models to a format that can be deployed to production environments.

TorchVision: A library of datasets and model architectures for computer vision tasks.

TorchText: A library for natural language processing tasks, such as text classification and language translation.

PyTorch Lightning: A lightweight wrapper for PyTorch that simplifies the process of building and training complex models.

Overall, PyTorch has become a popular tool in the machine learning community due to its flexibility, ease of use, and strong community support. It has been used to build state-of-the-art models in a variety of fields, including computer vision, natural language processing, and reinforcement learning.
![01_a_pytorch_workflow](https://user-images.githubusercontent.com/110838853/226788275-89bb2940-b100-42cd-a1a8-eba849aee218.png)


#### A collection of PyTorch notebooks for learning and practicing deep learning
https://github.com/dair-ai/pytorch_notebooks

####  Notebooks 
https://github.com/dair-ai/Mathematics-for-ML


## Computer Vision and CNN
Computer vision is a field of artificial intelligence that focuses on enabling machines to interpret and understand visual data from the world around us, such as images and videos. It involves developing algorithms and techniques that allow computers to analyze, process, and understand digital images and videos in a way that is similar to how humans perceive and interpret visual information.

Convolutional neural networks (CNNs) are a type of deep learning algorithm that are commonly used in computer vision tasks such as image recognition, object detection, and image segmentation. CNNs are inspired by the structure of the human visual system and are designed to automatically learn hierarchical representations of visual data.

CNNs are made up of multiple layers, including convolutional layers, pooling layers, and fully connected layers. The convolutional layers use filters to convolve over the input image, detecting features such as edges, textures, and shapes. The pooling layers then reduce the spatial size of the output feature maps, making the network more efficient and allowing it to focus on the most important features. The fully connected layers then take the output from the convolutional and pooling layers and use it to make a final classification or prediction.

CNNs have been shown to achieve state-of-the-art performance on a wide range of computer vision tasks and are widely used in industry and academia for image and video analysis.

![89175cnn_banner](https://user-images.githubusercontent.com/110838853/226784233-75acbbcd-7162-4a02-a2da-6b113d293c5a.png)


#### Computer vision Use cases
https://github.com/ashishpatel26/Computer-Vision-Industry-Use-Cases ## useful for the interview preparation and knowledge of CNN


## CNN
https://copyassignment.com/flower-classification-using-cnn/
### **Must Read Github Repository for CNN**
https://github.com/Avik-Jain/DeepLearning.ai-Summary/tree/master/4-%20Convolutional%20Neural%20Networks 

### CNN Course 

https://madewithml.com/courses/foundations/convolutional-neural-networks/

## YOLO

YOLO (You Only Look Once) is an object detection algorithm used in computer vision. It was first introduced in a paper by Joseph Redmon et al. in 2016. YOLO is designed to be fast and efficient, capable of processing images in real-time on a standard desktop GPU.

The YOLO algorithm works by dividing an input image into a grid of cells, and for each cell, predicting a set of bounding boxes and associated class probabilities. Each bounding box consists of four coordinates (x, y, width, height), which define the location and size of an object within the image. The class probabilities indicate the likelihood that the object within the bounding box belongs to a particular class, such as person, car, or bicycle.

One of the main advantages of YOLO is its speed and efficiency. By processing the entire image at once, YOLO is able to detect objects in real-time, making it useful for applications such as self-driving cars and real-time surveillance. Additionally, YOLO is able to detect small objects and objects with low contrast, which can be challenging for other object detection algorithms.

Since its initial release, several variants of YOLO have been developed, including YOLOv2, YOLOv3, and YOLOv4. These variants incorporate improvements such as feature extraction from multiple scales, better regularization techniques, and use of novel architectures such as the spatial pyramid pooling module.

Overall, YOLO has become a popular algorithm in the computer vision community due to its speed and accuracy, and has been used in a wide range of applications, including autonomous vehicles, security systems, and robotics.

![ezgif-3-fba2e64257](https://user-images.githubusercontent.com/110838853/226788844-c8ea00fd-85f3-4a4a-8fe5-7b47c078a27a.jpg)

#### YOLOv4 / Scaled-YOLOv4 / YOLO - Neural Networks for Object Detection (Windows and Linux version of Darknet )
https://github.com/AlexeyAB/darknet

#### YOLOv5 🚀 in PyTorch 
https://github.com/ultralytics/yolov5


### Resourcs for CNN,ML,NLP
https://github.com/ashishpatel26/ResourceBank_CV_NLP_MLOPS_2022   ###This repository offers a goldmine of materials for  computer vision, natural language processing, and machine learning operations.

####  Notebooks 
https://github.com/dair-ai/Mathematics-for-ML

### Explanation to key concepts in CNN
https://github.com/dair-ai/ML-Papers-Explained


##  Object Detection

Object detection is a computer vision technique used to locate and identify objects within an image or video. It involves identifying the presence of objects in an image or video and their corresponding class labels, as well as the precise location of each object within the image or video.

Object detection can be divided into two main stages:

Object Localization: The first stage involves finding the location of objects within the image or video. This is typically done by predicting a set of bounding boxes that enclose each object of interest. Each bounding box consists of four coordinates (x, y, width, height) that define the location and size of the object.

Object Classification: The second stage involves identifying the class label of each object within the bounding box. This is typically done by using a machine learning algorithm that has been trained on a dataset of labeled images or videos.

Object detection has many applications, including self-driving cars, surveillance systems, and robotics. It can also be used for tasks such as counting the number of people in a crowd or detecting anomalies in medical images. Object detection algorithms typically involve complex machine learning models, such as convolutional neural networks (CNNs), and are computationally intensive. However, recent advancements in hardware and software have made it possible to perform object detection in real-time on standard computing devices.
![Detected-with-YOLO--Schreibtisch-mit-Objekten](https://user-images.githubusercontent.com/110838853/226789288-7c81f8c1-b0a0-4a93-96a4-3f2ed8e3911e.jpg)


#### Collection of papers and other resources for object tracking and detection using deep learning
https://github.com/ashishpatel26/Deep-Learning-for-Tracking-and-Detection 



## Large Language Model(LLM)
#### CheatSheet-LLM
https://github.com/Abonia1/CheatSheet-LLM

## Data Analysics


## SQL




## other Resources To Help with Projects and Understanding Key Concepts

### 500 Projects of Deep learning and CNN and NLP 
https://github.com/ashishpatel26/500-AI-Machine-learning-Deep-learning-Computer-vision-NLP-Projects-with-code ##500 AI Machine learning Deep learning Computer vision NLP Projects with code

### All algorithms implementd on Python
https://github.com/TheAlgorithms/Python


## chatgpt-prompts
https://github.com/f/awesome-chatgpt-prompts

## Tableau



## NLP

![pnlp_0101](https://user-images.githubusercontent.com/110838853/226782983-768b069e-52ed-4442-8909-d9553ab8b61a.png)

#### NLP Course Notes and Videos
https://github.com/ashishpatel26/ML-Course-Notes

### Resourcs for CNN,ML,NLP
https://github.com/ashishpatel26/ResourceBank_CV_NLP_MLOPS_2022   ###This repository offers a goldmine of materials for  computer vision, natural language processing, and machine learning operations.


### Curated List of Papers on NLP
https://github.com/dair-ai/nlp_paper_summaries

####  Notebooks 
https://github.com/dair-ai/Mathematics-for-ML



## Transformers
Transformers are a type of machine learning architecture used primarily for natural language processing tasks such as language translation, sentiment analysis, and text classification. The Transformer architecture was first introduced in a 2017 paper called "Attention is All You Need" by Vaswani et al.

Traditionally, recurrent neural networks (RNNs) have been used for sequence-to-sequence tasks, but Transformers have gained popularity due to their ability to handle long-range dependencies in sequences more efficiently.

Transformers rely on self-attention mechanisms to determine which parts of a sequence are most relevant to each other. Self-attention allows the model to weigh the importance of different words or tokens in a sequence when generating an output.

Transformers have been used with great success in various natural language processing tasks and have even been applied to other domains such as image and audio processing. The pre-trained transformer models such as BERT, GPT-3, and T5 have achieved state-of-the-art results on a wide range of natural language processing benchmarks.
![Transformer-apps](https://user-images.githubusercontent.com/110838853/226783432-63a3ebfc-2789-47f9-bc1f-4dbb11f30a1e.jpg)


#### Study Guide to learn Transformers
https://github.com/dair-ai/Transformers-Recipe

#### Resources for Transformers
https://github.com/ashishpatel26/Treasure-of-Transformers ## Awesome Treasure of Transformers Models for Natural Language processing contains papers, videos, blogs, official repo along with colab Notebooks.

#### Transformers Course Notes and Videos
https://github.com/ashishpatel26/ML-Course-Notes

### Explanation to key concepts in Tranformers papers
https://github.com/dair-ai/ML-Papers-Explained

#### Medium Article for The Resources of Transformers
https://medium.com/nlplanet/two-minutes-nlp-20-learning-resources-for-transformers-1bbff88b7524 ##20 Learning Resources for Transformers


####  Notebooks 
https://github.com/dair-ai/Mathematics-for-ML

## Portfolio
#### A curated list of awesome GitHub Profile READMEs
https://github.com/abhisheknaiidu/awesome-github-profile-readme


## GAN
GAN stands for Generative Adversarial Networks, which is a type of deep learning model used in unsupervised machine learning tasks such as image generation, video generation, and text generation. GANs were first introduced by Ian Goodfellow in 2014.

A GAN consists of two neural networks: a generator and a discriminator. The generator takes random noise as input and produces an output, such as an image or a piece of text. The discriminator takes as input the output from the generator, as well as real samples from the training data, and tries to distinguish between the real and fake samples.

During training, the generator and discriminator are trained in a adversarial way. The generator tries to produce outputs that fool the discriminator into thinking they are real, while the discriminator tries to accurately distinguish between the real and fake samples. This process continues until the generator is able to produce samples that are indistinguishable from the real samples, and the discriminator is no longer able to distinguish between the real and fake samples.

GANs have been used for a wide range of applications, such as image synthesis, image editing, and data augmentation. They have shown remarkable performance in generating realistic images, and have been used to create images of faces, animals, and even furniture. GANs have also been used for other types of generative tasks, such as generating music, video, and speech.


![BP3_fig1-1024x390](https://user-images.githubusercontent.com/110838853/226784902-30e81abf-06d9-444f-adf2-096eeb8c3e4f.jpg)


####  Notebooks 
https://github.com/dair-ai/Mathematics-for-ML

## Graph Neural Networks (GNNs)
GNN stands for Graph Neural Network, which is a type of machine learning model designed to operate on graphs and other network data structures.

In a graph, nodes are connected by edges that represent relationships or connections between them. GNNs are designed to learn from and operate on these graph structures, allowing them to model complex relationships and dependencies between nodes.

GNNs work by propagating information through the graph structure, updating each node's representation based on its neighboring nodes and edges. This propagation process is typically performed through a series of message-passing steps that update node features based on the features of neighboring nodes and edges.

GNNs have been used in a variety of applications, such as social network analysis, recommendation systems, and drug discovery. They have been shown to be particularly effective in tasks where the relationships between data points are crucial for accurate predictions, such as in protein structure prediction and traffic prediction.

Some popular GNN architectures include Graph Convolutional Networks (GCNs), Graph Attention Networks (GATs), and GraphSAGE.
![GNN-03-scaled](https://user-images.githubusercontent.com/110838853/226785402-cec303e4-f1e2-414d-b2d6-41e1e086f749.jpg)

#### A study guide to learn about Graph Neural Networks (GNNs)
https://github.com/dair-ai/GNNs-Recipe  

## Linux


##Flask

## End to End Machine learning Projects Deployement
